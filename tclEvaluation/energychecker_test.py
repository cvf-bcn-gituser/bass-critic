
"""EnergyChecker_AllStems.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/164BliWxvoARK1_Q03_1A2sGKAsHrE7oK
"""

import madmom
from essentia.standard import *
from essentia import Pool, array
import essentia.standard as es
import matplotlib.pyplot as plt
import numpy as np
import IPython.display as ipd
import os

import IPython
import pickle
from pickle import load
from scipy.signal import find_peaks
import ipywidgets as widgets
from scipy import signal
from lxml import etree

import plotly.express as px
import scipy

import mir_eval
from mir_eval import *
from statistics import mean
import math
from math import sqrt
import pandas as pd
import sys
#sys.path.append(root)
ground_t_offsets_array = []
ground_t_onsets_array = []
ground_t_durations_array = []

# 2. Load the annotated onsets
ground_t_onsets_array=[]
ground_t_offsets_array=[]
#rhythm_filename = 'Billiejean_rhythm.csv'
rhythm_filenames = ['data/bass/Yellow_rhythm.csv','data/bass/Billiejean_rhythm.csv','data/bass/Justlooking_rhythm.csv','data/bass/Brown_rhythm.csv','data/bass/Roadrunner_rhythm.csv','data/bass/Walking_rhythm.csv']

col_list = ["onset", "muted", "offset"]
i= 0
df_array= []
while i < 6:#len(rhythm_filenames):
  print(i)
  df = pd.read_csv(rhythm_filenames[i], usecols=col_list)
  df_array.append(df)
  i+=1

stem_filenames = ['data/bass/2_bassX_Yellow_Coldplay_Bass Gr0.wav',
'data/bass/2_bassX_Billie Jean_Michael Jackson_BassGr1.wav',
'data/bass/2_bassX_Just Looking_Stereophonics_BassGr1.wav',
'data/bass/2_bassX_Brown Eyed Girl_Van Morrison_BassGr2.wav',
'data/bass/2_bassX_(I_m A) Road Runner_Jr Walker _ The Allstars_Bass Gr3.wav',
'data/bass/2_bassX_Walking On The Moon_Half.wav']

fs = 44100
raw_audio = []
i= 0
while i < 6:#len(stem_filenames)-1:
  audio_file = stem_filenames[i]
  raw = MonoLoader(filename = audio_file, sampleRate = fs)()
  raw = raw / np.max(np.abs(raw))
  raw_audio.append(raw)
  print(stem_filenames[i])
  print(len(raw_audio[i]))
  i+=1

def match_events(gt_onsets, onsets, matching_window_size):
    """
    Finds best matching pairs so
       - distance between elements is no greater than matching_window_size
       - sum of all distances is minimized

    """
    # In case of performance issues for big piecs,
    # we could try to use simpler/faster local algorithm.
    m = scipy.spatial.distance_matrix([[x] for x in gt_onsets], [[x] for x in onsets])
    # don't consider events which are out of matching window size
    big_distance = 10 ** 6
    m[m > matching_window_size] = big_distance
    
    row_ind, col_ind = scipy.optimize.linear_sum_assignment(m)
    result = []
    missing_notes= 0
    deviations = []
    for (x, y) in zip(row_ind, col_ind):
        if abs(onsets[y]-gt_onsets[x]) <= matching_window_size:
            result.append((x, y))
            deviations.append(gt_onsets[x] - onsets[y])
        else:
            missing_notes+=1
    return result,deviations,missing_notes

def f_measure(precision, recall):
    if precision == 0 and recall == 0:
        return 0.0
    return 2.0 * precision*recall / (precision + recall)

def evaluate_accuracy(gt_onsets, onsets, matching_window_size):
    matching,_,_= match_events(
        gt_onsets,
        onsets,
        matching_window_size)
    #print("DBG4")
    precision = float(len(matching)) / len(onsets)
    recall = float(len(matching)) / len(gt_onsets)
    f_measure_value = f_measure(precision, recall)
    return precision, recall, f_measure_value

"""The energy of a discrete time signal can be computed as:
\begin{equation}
energy = \sum_{n = 0}^{N-1} |x[n]|^2
\end{equation}
where x[n] refers to the discrete time signal sample at index n
"""

# Onset detection using Spectral Onset Processor blending ideas from Ramon
# myOnsetEnergyChecker
# Returns 
def myOnsetEnergyChecker(x,theFrameSize,theHopSize,thresh):
    '''
    Calculates the energy of an input audio signal
    '''
    NRG = [];
    #Main windowing and feature extraction loop
    for frame in FrameGenerator(x, frameSize = theFrameSize, hopSize = theHopSize):
        NRG.append(es.Energy()(frame))
    NRG = np.array(NRG)
    NRG = NRG / np.max(NRG)

    #Applying energy threshold to decide wave split boundaries
    split_decision_func = np.zeros_like(NRG)
    split_decision_func[NRG > thresh] = 1 # was 0.005
    #Setting segment boundaries
    #Inserting a zero at the beginning since we will decide the transitions using a diff function
    split_decision_func = np.insert(split_decision_func, 0, 0)
    diff_split_decision = np.diff(split_decision_func)
    #Start indexes: transition from 0 to 1
    start_indexes = np.nonzero(diff_split_decision > 0)[0] * hopSize/fs
    #Stop indexes: transition from 1 to 0
    stop_indexes = np.nonzero(diff_split_decision < 0)[0] * hopSize/fs

    return (start_indexes, stop_indexes,split_decision_func)

def multiple_hist(deviationsArray1,title_text1):

   a = np.array(deviationsArray1)
   m, s = mean(a), sqrt(mean(a*a))
   am = mean(abs(a))

   summary= title_text1+"ABS Mean: %f, Mean: %f, Dev. from 0: %f" %(am, m, s)
   print(summary)
   title =   title_text1

   plt.title(title)
   plt.figure(1, figsize=(9.5, 6))
   plt.hist(a)


def match_rhythm(df, onsets, offsets, matching_window_size):
    """
    Finds best matching pairs so
       - distance between elements is no greater than matching_window_size
       - sum of all distances is is minimized
       - also returns the fidelity ( conformance to 100% hit notes)
    """
    result = []
    missing_onset_notes= 0
    onset_deviation_array = []
    offset_deviation_array = []
    rhythm_deviation_array = []

    gt_onsets = df["onset"].tolist()
    gt_offsets = df["offset"].tolist()
    gt_muted= df["muted"].tolist()

    m = scipy.spatial.distance_matrix([[x] for x in gt_onsets], [[x] for x in onsets])
    # don't consider events which are out of matching window size
    big_distance = 10 ** 6
    m[m > matching_window_size] = big_distance

    row_ons, col_ons = scipy.optimize.linear_sum_assignment(m)
    jindex=-1
    for (xn, yn) in zip(row_ons, col_ons):
        jindex+=1
        if abs((onsets[yn]) - gt_onsets[xn]) <= matching_window_size:
            onset_deviation_array.append((onsets[yn]) - gt_onsets[xn]) 
            # We are within margin
            # Check if Muted
            if gt_muted[xn]=='Y':
              if abs((offsets[yn]) - gt_offsets[xn]) <= matching_window_size*2:
                 offset_deviation_array.append(offsets[yn] - gt_offsets[xn])
            else:
              offset_deviation_array.append(offsets[yn] - gt_onsets[xn+1])
        else:
            missing_onset_notes+=1
    return onset_deviation_array,offset_deviation_array,missing_onset_notes


i=0

windowSize = int(1024*1)
hopSize = int(512*1)
frameSize = windowSize

p = []
r = []
f = []
onsetIndexArray =  []
offsetIndexArray =  []
onset_deviationsArray  =  []
missing_onset_notesArray  =  []

thesholds = []

# yello 17.21 to 21  22.72 27.573 28.27 to 32
index = 1
onset_deviationsArray=[]
offset_deviationsArray=[]
threshIndex = [0.049,0.049,0.022, 0.04505,0.078, 0.1]
matching_window_size = 0.02  # MIREX reference
##### Do the Stems
#####################
#startTime=48.7,endTime=78
#####################
#audio = Trimmer(startTime=48.7,endTime=78)(raw_audio[5])#60.00507937
#audio = Trimmer(startTime= 48.76698413,endTime=75.9)(raw_audio[5])#60.00507937
#audio = Trimmer(startTime=0,endTime= 48.7)(raw_audio[0])#60.00507937
audio = Trimmer(startTime=17.21,endTime= 77.9)(raw_audio[0])
onsetIndex=[]
onset_array=[]
onsetIndex, offsetIndex,split_decision_func = myOnsetEnergyChecker(audio,frameSize,hopSize,0.06)
df = df_array[0]
onset_list= df["onset"].tolist() 
offset_list= df["offset"].tolist() 
#####################
#onset_list=onset_list[39:len(onset_list)-2]
#onset_list=onset_list[95:len(onset_list)-1]
#onset_list=onset_list[0:38]
onset_list=onset_list[0:177]
offset_list=offset_list[0:177]
#onsetIndex= array(onsetIndex)+ 48.7 #68.09142857 # 48.76698413
onsetIndex= array(onsetIndex)+ 17.21 #68.09142857 # 48.76698413
#onset_deviation_array,offset_deviation_array,missing_onset_notes = match_rhythm(df, onset_array_pitch, offset_array_pitch, matching_window_size)
#p,r,f=evaluate_accuracy(onset_list, onset_array_pitch, matching_window_size)
p,r,f=evaluate_accuracy(onset_list, onsetIndex, matching_window_size) 

print(round(p,3),round(r,3),round(f,3))

# Resutls for first parto f yellow with EC 0.83 0.746 0.786

index=0
while index < 6:# stem_filenames
    onsetIndex, offsetIndex,split_decision_func = myOnsetEnergyChecker(raw_audio[index],frameSize,hopSize,threshIndex[index])
    df = df_array[index]
    print(stem_filenames[index])
    onset_list= df["onset"].tolist()    
    p,r,f=evaluate_accuracy(onset_list, onsetIndex, matching_window_size)
    print(round(p,3),round(r,3),round(f,3))
    onset_deviations1,offset_deviations1,missing_onset_notes1 = match_rhythm(df,onsetIndex,offsetIndex,matching_window_size)
    #multiple_hist(onset_deviations1,"Onsets")
    #plt.show()
    #multiple_hist(offset_deviations1,"Offsets")
    #plt.show()
    index+=1
"""

index=0
while index < 6:# stem_filenames
    df = df_array[index]
    onset_list= df["onset"].tolist()    
    print("#############",len(onset_list), len(onset_array_pitch_array[index]))
    p,r,f=evaluate_accuracy(onset_list, onset_array_pitch_array[index], matching_window_size)
    print(round(p,3),round(r,3),round(f,3))
    onset_deviations1,offset_deviations1,missing_onset_notes1 = match_rhythm(df,onsetIndex,offsetIndex,matching_window_size)
    index+=1

# Taking 1 song (2Just Looking)
df = df_array[2]

gt_onsets = df["onset"].tolist()
results,devs,misses= match_events(gt_onsets, onsetIndex, 0.02)
print(devs)
#multiple_hist(devs,"Onsets")
#plt.show()
print("misses",misses,"Percentage Miss",100.0*misses/len(gt_onsets))

gt_offsets = df["offset"].tolist()
results,devs,misses= match_events(gt_offsets, offsetIndex, 0.03)
print(devs)
#multiple_hist(devs,"Offsets")
#plt.show()
print("misses",misses,"Percentage Miss",100.0*misses/len(gt_offsets))
"""